{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Enhancing Semantic Image Understanding with Fine-Tuned CLIP"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from PIL import Image\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from torchvision.datasets import CIFAR10\n",
    "from PIL import Image\n",
    "import random\n",
    "\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset creation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This dataset class facilitates the creation of pairs of images for training a model, particularly for tasks like contrastive learning or siamese networks, where pairs of similar and dissimilar samples are required. It's particularly useful for training models like CLIP, which benefit from learning to associate images based on their semantic content."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "# Function to resize image to the required size\n",
    "def resize_image(image, size):\n",
    "    return image.resize(size, Image.LANCZOS)\n",
    "\n",
    "class CIFAR10ClipDataset(Dataset):\n",
    "    def __init__(self, cifar_dataset, num_samples=None, transform=None):\n",
    "        self.cifar_dataset = cifar_dataset\n",
    "        self.transform = transform\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples if self.num_samples else len(self.cifar_dataset)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if self.num_samples:\n",
    "            idx1 = random.randint(0, len(self.cifar_dataset) - 1)\n",
    "            idx2 = random.randint(0, len(self.cifar_dataset) - 1)\n",
    "        else:\n",
    "            idx1 = idx\n",
    "            idx2 = random.randint(0, len(self.cifar_dataset) - 1)\n",
    "\n",
    "        img1, label1 = self.cifar_dataset[idx1]\n",
    "        img2, label2 = self.cifar_dataset[idx2]\n",
    "        \n",
    "        # Apply transforms if provided\n",
    "        if self.transform:\n",
    "            img1 = self.transform(img1)\n",
    "            img2 = self.transform(img2)\n",
    "\n",
    "        # Determine if the images are from the same category\n",
    "        if label1 == label2:\n",
    "            return img1, img2, torch.tensor(1)  # Pair from the same category\n",
    "        else:\n",
    "            return img1, img2, torch.tensor(0)  # Pair from different categories\n",
    "\n",
    "# Resize and normalize images\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize images to 224x224\n",
    "    transforms.ToTensor(),           # Convert images to PyTorch tensors\n",
    "])\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "cifar_dataset = CIFAR10(root='./data', train=True, download=True, transform=None)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Finetuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function effectively encourages the model to learn embeddings such that similar pairs have high cosine similarity values (close to 1) and dissimilar pairs have low cosine similarity values (close to -1). It's suitable for tasks like contrastive learning, where the model learns to distinguish between pairs of samples based on their semantic similarity.\n",
    "\n",
    "The loss is computed as follows:\n",
    "\n",
    "- For similar pairs (where labels is 1), the loss encourages the similarity value to be close to 1. Hence, (1 - similarities) penalizes low similarity scores.\n",
    "- For dissimilar pairs (where labels is 0), the loss encourages the similarity value to be close to -1. Hence, (1 - similarities) penalizes high similarity scores.\n",
    "- The loss is computed as the mean of these penalties across all pairs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "# Create DataLoader\n",
    "cifar_clip_dataset = CIFAR10ClipDataset(cifar_dataset,1000, transform=transform)\n",
    "batch_size = 128\n",
    "dataloader = DataLoader(cifar_clip_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# Custom loss function\n",
    "class CustomLoss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CustomLoss, self).__init__()\n",
    "\n",
    "    def forward(self, emb1, emb2, labels):\n",
    "        # Calculate cosine similarity between embeddings\n",
    "        similarities = torch.nn.functional.cosine_similarity(emb1, emb2)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = torch.mean((1 - labels) * similarities + labels * (1 - similarities))\n",
    "        return loss\n",
    "\n",
    "# Define the loss function\n",
    "criterion = CustomLoss()\n",
    "\n",
    "# Define the optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-6)\n",
    "\n",
    "# Move model to appropriate device\n",
    "model.to(device)\n",
    "\n",
    "epochs = 150\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(epochs):  # You can adjust the number of epochs\n",
    "    running_loss = 0.0\n",
    "    \n",
    "    for img1, img2, labels in dataloader:\n",
    "        # Move images to device\n",
    "        img1, img2, labels = img1.to(device), img2.to(device), labels.to(device)\n",
    "        \n",
    "        # Generate image embeddings\n",
    "        emb1 = model.get_image_features(img1)\n",
    "        emb2 = model.get_image_features(img2)\n",
    "        \n",
    "        # Calculate loss\n",
    "        loss = criterion(emb1, emb2, labels)\n",
    "        \n",
    "        # Zero gradients, backward pass, and optimization step\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "    \n",
    "    # Print average loss for the epoch\n",
    "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {running_loss/len(dataloader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import CLIPProcessor, CLIPModel\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Function to load and preprocess image\n",
    "def resize_image(image_path, target_size=(224, 224)):\n",
    "    # Open the image using PIL\n",
    "    img = Image.open(image_path).convert(\"RGB\")\n",
    "    # Resize the image to the target size\n",
    "    resized_img = img.resize(target_size, Image.Resampling.LANCZOS)\n",
    "    return resized_img\n",
    "\n",
    "# Define a function to load and preprocess images from file paths\n",
    "def load_and_preprocess_images(image_paths):\n",
    "    images = [resize_image(path) for path in image_paths]\n",
    "    return images\n",
    "\n",
    "# Load images\n",
    "image_paths = [\"/workspace/data/Untitled.jpg\",\"/workspace/data/avion-800_1.png\",\"/workspace/data/2021-best-cars-ford-mustang-hero-desktop.jpg\",\"/workspace/data/kitty-cat-kitten-pet-45201.jpeg\",\"/workspace/data/Puerto_rican-Paso-Fino-Horse-chestnut.jpg\",\"/workspace/data/P-51_Mustang_edit1.jpg\",\"/workspace/data/Comment-le-secteur-automobile-prepare-la-voiture-durable.png\",\"/workspace/data/Asana3808_Dashboard_Standard.jpg\"] # Replace with paths to your images\n",
    "\n",
    "# Load CLIP model from Hugging Face Transformers\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Generate image embeddings using the original CLIP model\n",
    "image_embeddings = []\n",
    "for image_path in image_paths:\n",
    "    images = load_and_preprocess_images([image_path])  # Pass image path as a list\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        outputs = clip_model.get_image_features(**inputs)\n",
    "    image_embeddings.append(outputs)\n",
    "\n",
    "# Generate image embeddings using the fine-tuned model (assuming `model` is your fine-tuned model)\n",
    "fine_tuned_image_embeddings = []\n",
    "for image_path in image_paths:\n",
    "    images = load_and_preprocess_images([image_path])  # Pass image path as a list\n",
    "    with torch.no_grad():\n",
    "        inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(\"cuda\")\n",
    "        outputs = model.get_image_features(**inputs)\n",
    "    fine_tuned_image_embeddings.append(outputs)\n",
    "\n",
    "# Compute similarity scores\n",
    "def compute_similarity_matrix(embeddings):\n",
    "    num_images = len(embeddings)\n",
    "    similarity_matrix = torch.zeros((num_images, num_images))\n",
    "    \n",
    "    for i, embedding1 in enumerate(embeddings):\n",
    "        for j, embedding2 in enumerate(embeddings):\n",
    "            similarity = torch.cosine_similarity(embedding1, embedding2).item()\n",
    "            similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "original_similarities = compute_similarity_matrix(image_embeddings)\n",
    "fine_tuned_similarities = compute_similarity_matrix(fine_tuned_image_embeddings)\n",
    "\n",
    "# Function to plot the similarity matrix with images\n",
    "def plot_similarity_matrix(similarity_matrix, image_paths, title):\n",
    "    num_images = len(image_paths)\n",
    "    fig, ax = plt.subplots(figsize=(15, 15))\n",
    "    similarity = similarity_matrix.numpy()\n",
    "    ax.imshow(similarity, vmin=0, vmax=1, cmap='viridis')\n",
    "    \n",
    "    ax.set_xticks(np.arange(num_images))\n",
    "    ax.set_yticks(np.arange(num_images))\n",
    "    \n",
    "    # Plot images on top and left side\n",
    "    for i in range(num_images):\n",
    "        img_top = Image.open(image_paths[i])\n",
    "        img_left = Image.open(image_paths[i])\n",
    "        \n",
    "        ax.imshow(img_top, extent=(i - 0.5, i + 0.5, -1.5, -0.5), origin=\"lower\")\n",
    "        ax.imshow(img_left, extent=(-1.5, -0.5, i - 0.5, i + 0.5), origin=\"lower\")\n",
    "    \n",
    "    for x in range(similarity.shape[1]):\n",
    "        for y in range(similarity.shape[0]):\n",
    "            ax.text(x, y, f\"{similarity[y, x]:.4f}\", ha=\"center\", va=\"center\", size=12, color='white')\n",
    "    \n",
    "    for side in [\"left\", \"top\", \"right\", \"bottom\"]:\n",
    "        ax.spines[side].set_visible(False)\n",
    "    \n",
    "    ax.set_xlim([-1.5, num_images - 0.5])\n",
    "    ax.set_ylim([num_images - 0.5, -1.5])\n",
    "    ax.set_title(title, size=20)\n",
    "    \n",
    "    plt.show()\n",
    "\n",
    "# Plot and display the similarity matrices\n",
    "plot_similarity_matrix(original_similarities, image_paths, \"Original CLIP Model Similarities\")\n",
    "plot_similarity_matrix(fine_tuned_similarities, image_paths, \"Fine-tuned CLIP Model Similarities\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See the results in the readme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define CIFAR-10 classes\n",
    "cifar10_classes = ['airplane', 'automobile', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "# Load CIFAR-10 dataset\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "test_dataset = CIFAR10(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# Create a subset of the test dataset with only 1000 images\n",
    "subset_indices = list(range(1000))\n",
    "test_subset = Subset(test_dataset, subset_indices)\n",
    "test_loader = DataLoader(test_subset, batch_size=64, shuffle=False)\n",
    "\n",
    "# Load CLIP model from Hugging Face Transformers\n",
    "clip_model = CLIPModel.from_pretrained(\"openai/clip-vit-base-patch32\").to(\"cuda\")\n",
    "processor = CLIPProcessor.from_pretrained(\"openai/clip-vit-base-patch32\")\n",
    "\n",
    "# Placeholder for the fine-tuned model (replace 'fine_tuned_model' with your actual fine-tuned model)\n",
    "fine_tuned_model = model  # Replace with your actual fine-tuned model\n",
    "\n",
    "def get_classwise_embeddings(model, data_loader, device):\n",
    "    classwise_embeddings = {cls: [] for cls in range(10)}\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for images, labels in data_loader:\n",
    "            images = images.to(device)\n",
    "            # Convert images from [0, 1] range to [0, 255] range\n",
    "            images = images * 255.0\n",
    "            inputs = processor(images=images, return_tensors=\"pt\", padding=True).to(device)\n",
    "            outputs = model.get_image_features(**inputs)\n",
    "            \n",
    "            for output, label in zip(outputs, labels):\n",
    "                classwise_embeddings[label.item()].append(output.cpu())\n",
    "    \n",
    "    # Compute mean embeddings for each class\n",
    "    for cls in classwise_embeddings:\n",
    "        classwise_embeddings[cls] = torch.stack(classwise_embeddings[cls]).mean(dim=0)\n",
    "    \n",
    "    return classwise_embeddings\n",
    "\n",
    "# Get embeddings for both models\n",
    "original_classwise_embeddings = get_classwise_embeddings(clip_model, test_loader, \"cuda\")\n",
    "fine_tuned_classwise_embeddings = get_classwise_embeddings(fine_tuned_model, test_loader, \"cuda\")\n",
    "\n",
    "def compute_mean_similarity_matrix(classwise_embeddings):\n",
    "    num_classes = len(classwise_embeddings)\n",
    "    similarity_matrix = torch.zeros((num_classes, num_classes))\n",
    "    \n",
    "    for i in range(num_classes):\n",
    "        for j in range(num_classes):\n",
    "            similarity = torch.cosine_similarity(classwise_embeddings[i], classwise_embeddings[j], dim=0).item()\n",
    "            similarity_matrix[i, j] = similarity\n",
    "    \n",
    "    return similarity_matrix\n",
    "\n",
    "original_similarity_matrix = compute_mean_similarity_matrix(original_classwise_embeddings)\n",
    "fine_tuned_similarity_matrix = compute_mean_similarity_matrix(fine_tuned_classwise_embeddings)\n",
    "\n",
    "# Function to plot the similarity matrix\n",
    "def plot_similarity_matrix(similarity_matrix, class_names, title):\n",
    "    fig, ax = plt.subplots(figsize=(10, 10))\n",
    "    similarity = similarity_matrix.numpy()\n",
    "    im = ax.imshow(similarity, vmin=0, vmax=1, cmap='viridis')\n",
    "    \n",
    "    ax.set_xticks(np.arange(len(class_names)))\n",
    "    ax.set_yticks(np.arange(len(class_names)))\n",
    "    ax.set_xticklabels(class_names, rotation=45, ha='right')\n",
    "    ax.set_yticklabels(class_names)\n",
    "    \n",
    "    # Display similarity scores\n",
    "    for i in range(len(class_names)):\n",
    "        for j in range(len(class_names)):\n",
    "            text = ax.text(j, i, f\"{similarity[i, j]:.2f}\", ha=\"center\", va=\"center\", color=\"white\")\n",
    "    \n",
    "    fig.colorbar(im, ax=ax)\n",
    "    ax.set_title(title, size=20)\n",
    "    plt.show()\n",
    "\n",
    "# Plot and display the similarity matrices\n",
    "plot_similarity_matrix(original_similarity_matrix, cifar10_classes, \"Original CLIP Model Mean Similarities\")\n",
    "plot_similarity_matrix(fine_tuned_similarity_matrix, cifar10_classes, \"Fine-tuned CLIP Model Mean Similarities\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
